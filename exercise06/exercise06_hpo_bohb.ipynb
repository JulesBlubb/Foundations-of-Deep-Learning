{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sixth Exercise (HPO)\n",
    "\n",
    "This exercise focuses on hyperparameter optimization with neural networks.\n",
    "\n",
    "We will\n",
    "- define hyperparameter-configuration search-spaces\n",
    "- train deep learning models with various hyper parameters\n",
    "- use random search as a basic hyperparameter optimizer\n",
    "- use BOHB as an advanced hyperparameter optimizer\n",
    "\n",
    "It's in the nature of hyperparameter optimization, that you'll have to train a lot of models. \n",
    "Therefore execution time will be longer in this exercise, running the completed notebook takes about 30 minutes on a tutor's 3 year old laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install HpBandster and torchvision\n",
    "\n",
    "You need to install two more python packages for this exercise.\n",
    "\n",
    "- [torchvision](https://pytorch.org/docs/stable/torchvision/) provides utility methods for pytorch.\n",
    "- [HpBandSter](https://github.com/automl/HpBandSter) is a fast, parallel implementation of several hyperparameter optimizers.  \n",
    "   We can define even complex hyperparameter search spaces with [ConfigSpace](https://github.com/automl/ConfigSpace), which comes along HpBandSter as dependency.\n",
    "\n",
    "\n",
    "Install with anaconda/conda\n",
    "```\n",
    "conda install torchvision\n",
    "conda install hpbandster\n",
    "```\n",
    "\n",
    "or with plain python\n",
    "```\n",
    "pip3 install torchvision\n",
    "pip3 install hpbandster\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def load_mnist_minibatched(batch_size: int, n_train: int = 8192, n_valid: int = 1024,\n",
    "                           valid_test_batch_size: int = 1024) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='../data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='../data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(range(n_train))\n",
    "    validation_sampler = SubsetRandomSampler(range(n_train, n_train+n_valid))\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    validation_loader = DataLoader(dataset=train_dataset, batch_size=valid_test_batch_size,\n",
    "                                   sampler=validation_sampler)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=valid_test_batch_size, \n",
    "                                              shuffle=False)\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model: nn.Module, data_loader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            output = model(x)\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "    accuracy = correct / len(data_loader.sampler)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "\n",
    "Here we get hands on hyperparameter optimization using random search.\n",
    "\n",
    "### Model and Hyperparameter Space\n",
    "\n",
    "First we define a configurable model and a hyperparameter space. You learn how to use *ConfigSpace* to define the hyperparameters, by looking at [this example](https://automl.github.io/SMAC3/stable/quickstart.html#using-smac-in-python-svm).   \n",
    "\n",
    "**Task:** Complete the functions as described in the docstrings.\n",
    "\n",
    "**Hint:** The `CS.GreaterThanCondition(conditioned_hyperparameter, lefthand_side, righthand_side)` method might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import deque\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\" Flatten feature dimension to (batch_size, feature_dim, 1).\n",
    "            \n",
    "        Note: This layer doesn't exist in pytorch but allows us to create\n",
    "              the model using `Sequential` only.\n",
    "    \"\"\"       \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = x.shape\n",
    "        batch_size = x.shape[0]\n",
    "        flat = x.reshape(batch_size, -1)\n",
    "        print(x.shape)\n",
    "        print(flat.shape)\n",
    "        return flat\n",
    "    \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        old_shape = self.input_cache\n",
    "        return grad.reshape(old_shape)\n",
    "\n",
    "def get_conv_model(num_filters_per_layer: List[int]) -> nn.Module:\n",
    "    \"\"\"Builds a deep convolutional model with various number of convolution\n",
    "       layers for MNIST input using pytorch.\n",
    "    \n",
    "    for each element in num_filters_per_layer:\n",
    "        convolution (conv_kernel_size, num_filters, stride=1, padding=0)\n",
    "        relu\n",
    "        max pool    (pool_kernel_size, stride=1)\n",
    "    linear\n",
    "    log softmax\n",
    "    \"\"\"\n",
    "    assert len(num_filters_per_layer) > 0, \"len(num_filters_per_layer) should be greater than 0\"\n",
    "    pool_kernel_size = 2\n",
    "    conv_kernel_size = 3\n",
    "    \n",
    "    # START ################\n",
    "    \n",
    "    #conv = nn.Sequential(\n",
    "    #    nn.Conv2d(in_channels=1, out_channels= num_filters_per_layer[0], kernel_size=conv_kernel_size, stride=1, padding=0),\n",
    "    #    nn.ReLU(),\n",
    "    #    nn.MaxPool2d(kernel_size=pool_kernel_size, stride=1)\n",
    "    #)\n",
    "    #\n",
    "    #for l in range(1, len(num_filters_per_layer)):\n",
    "    #    conv.add_module('conv2',nn.Conv2d(in_channels=num_filters_per_layer[l-1], out_channels=num_filters_per_layer[l], kernel_size=conv_kernel_size, stride=1, padding=0))\n",
    "    #    conv.add_module('relu2',nn.ReLU())\n",
    "    #    conv.add_module('maxpool2',nn.MaxPool2d(kernel_size=pool_kernel_size, stride=1))\n",
    "    ##############TODO####################################################################################################\n",
    "    \n",
    "    #modules = conv.modules()\n",
    "\n",
    "    #dd = deque(modules, maxlen=1)\n",
    "    #last_element = dd.pop()\n",
    "    ##print(type(last_element))\n",
    "    #\n",
    "    #lin_inputs = last_element\n",
    "    \n",
    "    #print(lin_inputs)\n",
    "    \n",
    "    #conv.add_module('linear', nn.Linear(num_filters_per_layer[-1], 10))\n",
    "    #conv.add_module('logsoftmax', nn.LogSoftmax())\n",
    "    \n",
    "    d = OrderedDict([\n",
    "        ('conv0', nn.Conv2d(in_channels=1, out_channels= num_filters_per_layer[0], kernel_size=conv_kernel_size, stride=1, padding=0)),\n",
    "        ('relu0', nn.ReLU()),\n",
    "        ('maxpool0', nn.MaxPool2d(kernel_size=pool_kernel_size, stride=1))\n",
    "    ])\n",
    "    \n",
    "    out_conv = (28 - conv_kernel_size ) + 1\n",
    "    out_max = (out_conv - pool_kernel_size) + 1\n",
    "    \n",
    "    for l in range(1, len(num_filters_per_layer)):\n",
    "        d['conv' + str(l)] = nn.Conv2d(in_channels=num_filters_per_layer[l-1], out_channels=num_filters_per_layer[l], kernel_size=conv_kernel_size, stride=1, padding=0)\n",
    "        d['relu' + str(l)] = nn.ReLU()\n",
    "        d['maxpool' + str(l)] = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=1)\n",
    "        out_conv = (out_max - conv_kernel_size ) + 1\n",
    "        out_max = (out_conv - pool_kernel_size) + 1\n",
    "    \n",
    "    print(out_conv, out_max)\n",
    "    \n",
    "    d['flatten'] = Flatten()\n",
    "    d['linear'] =  nn.Linear(num_filters_per_layer[-1] * out_max * out_max, 10)\n",
    "    d['logsoftmax'] = nn.LogSoftmax()\n",
    "    \n",
    "    conv = nn.Sequential(d)  \n",
    "    \n",
    "    return conv\n",
    "    \n",
    "    \n",
    "    # End ################\n",
    "\n",
    "\n",
    "def get_configspace() -> CS.ConfigurationSpace:\n",
    "    \"\"\" Define a conditional hyperparameter search-space.\n",
    "    \n",
    "    hyperparameters:\n",
    "      lr              from 1e-6 to 1e-0 (log, float)\n",
    "      num_filters_1   from    2 to    8 (int)\n",
    "      num_filters_2   from    2 to    8 (int)\n",
    "      num_conv_layers from    1 to    2 (int)\n",
    "    \n",
    "    conditions: \n",
    "      include num_filters_2 only if num_conv_layers > 1\n",
    "    \"\"\"\n",
    "    cs = CS.ConfigurationSpace()\n",
    "    # START ################\n",
    "    \n",
    "    # learning rate\n",
    "    lr = CSH.UniformFloatHyperparameter('learning_rate', lower=1e-6, upper=1e-0, log=True)\n",
    "    # num_filters_1\n",
    "    num_filters_1 = CSH.UniformIntegerHyperparameter('num_filters_1', lower=2, upper=8)\n",
    "    # num_filters_2\n",
    "    num_filters_2 = CSH.UniformIntegerHyperparameter('num_filters_2', lower=2, upper=8)\n",
    "    # num_conv_layers\n",
    "    num_conv_layers = CSH.UniformIntegerHyperparameter('num_conv_layers', lower=1, upper=2)\n",
    "    #condition\n",
    "    cond = CS.GreaterThanCondition(num_filters_2, num_conv_layers, 1)\n",
    "    \n",
    "    cs.add_hyperparameters([lr, num_filters_1, num_filters_2, num_conv_layers])\n",
    "    cs.add_condition(cond)\n",
    "    \n",
    "    # End ################\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with configuration\n",
    "\n",
    "A single sample from your hyperparameter space is a *Configuration*. You can use the configuration similar to a dictionary, it supports *config.keys(), config.values(), value = config[key], key in config, …* .\n",
    "You can iterate a *DataLoader* to access (data, label) batches.\n",
    "\n",
    "**Note:** If a condition isn't met, the conditional hyperparameter isn't included in the configuration.\n",
    "\n",
    "**Task:** Complete the function to run a model like defined by the configuration. The function should return the model and the *validation error* for each epoch. You can use *evaluate_accuracy* (defined above), don't forget to switch between train and eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conv_model(config: CS.Configuration, epochs: int, train_loader: DataLoader,\n",
    "                   validation_loader: DataLoader) -> Tuple[nn.Module, List[float]]:    \n",
    "    \"\"\" Run and evaluate a model from get_conv_model with NLLLoss and SGD.\n",
    "    \"\"\"\n",
    "    # START ################\n",
    "    # retrieve the number of filters from the config and create the model\n",
    "    num_filter = []\n",
    "    \n",
    "    num_filter.append(config['num_filters_1'])\n",
    "    \n",
    "    if config['num_conv_layers'] > 1:\n",
    "        num_filter.append(config['num_filters_2'])\n",
    "        \n",
    "    model = get_conv_model(num_filter)\n",
    "      \n",
    "    # define loss and optimizer\n",
    "    lr = config['learning_rate']\n",
    "    \n",
    "    loss = nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    # train the model for `epochs` and save the validation error for each epoch in\n",
    "    # val_errors\n",
    "    \n",
    "    val_errors = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for b in train_loader: \n",
    "            optimizer.zero_grad()\n",
    "            loss(model(b[0]), b[1]).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_errors.append(1 - evaluate_accuracy(model, validation_loader))\n",
    "\n",
    "    # End ################\n",
    "    return model, val_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run models with various, random hyperparameter configurations. Don't forget to store all the configuration and validation errors for further evaluation.\n",
    "\n",
    "**Tipp:** *ConfigSpace* objects have a *.sample_configuration()* function to sample a random configuration.\n",
    "\n",
    "**Task:** Run *n_random_samples* models for *n_epochs* and store the tuple `(model, config, val_errors)` in `results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  learning_rate, Value: 0.00032917291411869667\n",
      "  num_conv_layers, Value: 2\n",
      "  num_filters_1, Value: 4\n",
      "  num_filters_2, Value: 2\n",
      "\n",
      "26 25\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Megan\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:91: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([512, 6, 25, 25])\n",
      "torch.Size([512, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([512, 6, 25, 25])\n",
      "torch.Size([512, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n",
      "torch.Size([512, 6, 25, 25])\n",
      "torch.Size([512, 3750])\n",
      "torch.Size([32, 6, 25, 25])\n",
      "torch.Size([32, 3750])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-fe2320e0b75e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# START TODO ################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_configuration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrun_conv_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_configuration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-19733cc29770>\u001b[0m in \u001b[0;36mrun_conv_model\u001b[1;34m(config, epochs, train_loader, validation_loader)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \"\"\"\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_random_samples = 18\n",
    "n_epochs = 9\n",
    "cs = get_configspace()\n",
    "train_loader, validation_loader, _ = load_mnist_minibatched(batch_size=32, n_train=4096, n_valid=512)\n",
    "\n",
    "# START TODO ################\n",
    "print(cs.sample_configuration())\n",
    "run_conv_model(cs.sample_configuration(), n_epochs, train_loader, validation_loader)\n",
    "\n",
    "results = []\n",
    "\n",
    "for samp in range(n_random_samples):\n",
    "    conf = cs.sample_configuration()\n",
    "    model, val_errors = run_conv_model(conf, n_epochs, train_loader, validation_loader)\n",
    "    results.append((model, conf, val_errors))\n",
    "\n",
    "\n",
    "# End TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate \n",
    "\n",
    "Now we should evaluate the previous runs. Evaluation in hyperparameter optimization can mean two different things: On the one hand, we might be only interested in the model with the best performance. On the other hand, we might want to find the best hyperparameter configuration to then train a model with these hyper-parameters (but with e.g. more epochs).\n",
    "\n",
    "**Task:** Print the model and final validation error of the best model in `results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "\n",
    "# END TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further investigate which hyperparameters work well and which don't.\n",
    "\n",
    "**Task:** Print a scatter plot of learning rate (x) and number of filters (sum over layers, y). Scale the size of the scatter points by the error in the last epoch (10 to 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "\n",
    "# END TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What pattern do you see? Why might it occur?\n",
    "\n",
    "**Answer:** **TODO** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the final error, let's now have a look at the training error.\n",
    "\n",
    "**Task:** Plot error curves (error per epoch) for all your configurations in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "\n",
    "# END TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:** How could you detect configurations with a low error earlier/faster? Why could this be problematic? \n",
    "\n",
    "**Answers:** **TODO** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOHB\n",
    "\n",
    "Here we will use the more advanced hyperparameter optimizer [BOHB](https://www.automl.org/blog_bohb/) (Bayesian Optimization with Hyperband).\n",
    "Based on [Hyperband](https://arxiv.org/pdf/1603.06560.pdf), BOHB evaluates configurations on your model with increasing budgets. In the context of Deep Learning, budget can be the number of epochs or the number of training samples. In lower budget evaluations, BOHB can look at more configurations. Full budget evaluations avoid missing configurations which are poor at the beginnning but good at the end (and vice versa). \n",
    "At the start of a run, BOHB samples configurations randomly. After some time, BOHB then uses a bayesian model (based on Parzen Tree Estimators), sampling only promising configs.\n",
    "\n",
    "This exercise part is based on the [HpBandSter Examples](https://automl.github.io/HpBandSter/build/html/auto_examples/index.html) and the [HpBandSter Documentation](https://automl.github.io/HpBandSter/). *HpBandSter* provides a fast implementation of *Randomsearch*, *Hyperband* and *BOHB*. The optimization can easily be distributed between multiple cores or even multiple computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "from hpbandster.optimizers import BOHB\n",
    "\n",
    "logging.getLogger('hpbandster').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Worker\n",
    "\n",
    "The worker defines the hyperparameter problem which we try to optimize.\n",
    "*compute(...)* should - for a given configuration and budget - return a loss which the hyperparameter optimizer tries to minimize. In our case, we can use the number of epochs as budget and the validation error as loss. \n",
    "As best practice, we define the configuration space also in the worker.\n",
    "\n",
    "If you need help, you might get some inspiration from the [HpBandSter Pytorch Worker Example](https://automl.github.io/HpBandSter/build/html/auto_examples/example_5_pytorch_worker.html#sphx-glr-auto-examples-example-5-pytorch-worker-py).\n",
    "\n",
    "**Task:** Complete the methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchWorker(Worker):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_loader, self.validation_loader, self.test_loader =\\\n",
    "            load_mnist_minibatched(batch_size=32, n_train=4096, n_valid=512)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model(config: CS.Configuration) -> nn.Module:\n",
    "        \"\"\" Define a configurable convolution model.\n",
    "            \n",
    "        See description of get_conv_model above for more details on the model.\n",
    "        \"\"\"\n",
    "        # START TODO ################\n",
    "        raise NotImplementedError\n",
    "  \n",
    "        # END TODO ################\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_configspace() -> CS.Configuration:\n",
    "        \"\"\" Define a conditional hyperparameter search-space.\n",
    "    \n",
    "        hyperparameters:\n",
    "          num_filters_1   from    4 to   32 (int)\n",
    "          num_filters_2   from    4 to   32 (int)\n",
    "          num_filters_3   from    4 to   32 (int)\n",
    "          num_conv_layers from    1 to    3 (int)\n",
    "          lr              from 1e-6 to 1e-1 (float, log)\n",
    "          sgd_momentum    from 0.00 to 0.99 (float)\n",
    "          optimizer            Adam or  SGD (categoric)\n",
    "          \n",
    "        conditions: \n",
    "          include num_filters_2 only if num_conv_layers > 1\n",
    "          include num_filters_3 only if num_conv_layers > 2\n",
    "          include sgd_momentum  only if       optimizer = SGD\n",
    "        \"\"\"\n",
    "        # START ################\n",
    "        cs = CS.ConfigurationSpace()\n",
    "        \n",
    "        # num_filters_1\n",
    "        num_filters_1 = CSH.UniformIntegerHyperparameter('num_filters_1', lower=4, upper=32)\n",
    "        # num_filters_2\n",
    "        num_filters_2 = CSH.UniformIntegerHyperparameter('num_filters_2', lower=4, upper=32)\n",
    "        # num_filters_3\n",
    "        num_filters_3 = CSH.UniformIntegerHyperparameter('num_filters_3', lower=4, upper=32)\n",
    "        # num_conv_layers\n",
    "        num_conv_layers = CSH.UniformIntegerHyperparameter('num_conv_layers', lower=1, upper=3)\n",
    "\n",
    "        # learning rate\n",
    "        lr = CSH.UniformFloatHyperparameter('learning_rate', lower=1e-6, upper=1e-1, log=True)\n",
    "        # sgd momentum\n",
    "        sgd_momentum = CSH.UniformFloatHyperparameter('sgd_momentum', lower=0.00, upper=0.99)\n",
    "        #optimizer\n",
    "        optimizer = CSH.CategoricalHyperparameter('optimizer', ['Adam', 'SGD'])\n",
    "        \n",
    "        #condition\n",
    "        cond1 = CS.GreaterThanCondition(num_filters_2, num_conv_layers, 1)\n",
    "        cond2 = CS.GreaterThanCondition(num_filters_3, num_conv_layers, 2)\n",
    "        cond3 = CS.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n",
    "\n",
    "        cs.add_hyperparameters([num_filters_1, num_filters_2, num_filters_3, num_conv_layers, lr, sgd_momentum, optimizer])\n",
    "        cs.add_condition([cond1, cond2, cond3])\n",
    "\n",
    "        return cs\n",
    "  \n",
    "        # END ################\n",
    "\n",
    "    def compute(self, config: CS.Configuration, budget: float, working_directory: str,\n",
    "                *args, **kwargs) -> float:\n",
    "        \"\"\"Evaluate a function with the given config and budget and return a loss.\n",
    "        \n",
    "        Bohb tries to minimize the returned loss.\n",
    "        \n",
    "        In our case the function is the training and validation of a model,\n",
    "        the budget is the number of epochs and the loss is the validation error.\n",
    "        \"\"\"\n",
    "        model = self.get_model(config)\n",
    "        \n",
    "        # START TODO ################\n",
    "\n",
    "        # END TODO ################\n",
    "        \n",
    "        train_accuracy = evaluate_accuracy(model, self.train_loader)\n",
    "        validation_accuracy = evaluate_accuracy(model, self.validation_loader)\n",
    "        test_accuracy = evaluate_accuracy(model, self.test_loader)\n",
    "        \n",
    "        return ({\n",
    "                'loss': 1 - validation_accuracy,  # remember: HpBandSter minimizes the loss!\n",
    "                'info': {'test_accuracy': test_accuracy,\n",
    "                         'train_accuracy': train_accuracy,\n",
    "                         'valid_accuracy': validation_accuracy,\n",
    "                         'model': str(model)}\n",
    "                })\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's best practice to do a quick sanity check of our worker with a low budget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.curdir\n",
    "# minimum budget that BOHB uses\n",
    "min_budget = 1\n",
    "# largest budget BOHB will use\n",
    "max_budget = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker = PyTorchWorker(run_id='0')\n",
    "cs = worker.get_configspace()\n",
    "\n",
    "config = cs.sample_configuration().get_dictionary()\n",
    "print(config)\n",
    "\n",
    "res = worker.compute(config=config, budget=min_budget, working_directory=working_dir)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run BOHB\n",
    "\n",
    "We now run the hyperparameter search with BOHB and the worker which we defined above and save the result to disk. \n",
    "Try to understand what happens. *HpBandSter* allows to start additional workers on the same or remote devices to parallelize the executions, that's why we need to setup some network stuff (nameserver, nic, host, port, …). If you're interested, you can check this out in the [HpBandSter Examples](https://automl.github.io/HpBandSter/build/html/auto_examples/index.html), but it is beyond the scope of this exercise.\n",
    "\n",
    "**Note:** The code below will try 60 different configurations. Some of them are executed at multiple budgets, which results in about 80 model training. Therefore it might take a while (15-45 minutes on a laptop). If you are interested in how BOHB works, checkout [BOHB (Falkner et al. 2018)](http://proceedings.mlr.press/v80/falkner18a.html).\n",
    "\n",
    "**Note 2:** You can see the progress in the debug output below. The configuration identifier (called `config_id` in the docs) is a three-tuple `(current iteration, resampling in case of error, sample)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = os.path.join(working_dir, 'bohb_result.pkl')\n",
    "nic_name = 'lo'\n",
    "port = 0\n",
    "run_id = 'bohb_run_1'\n",
    "n_bohb_iterations = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Start a nameserver\n",
    "    host = hpns.nic_name_to_host(nic_name)\n",
    "    ns = hpns.NameServer(run_id=run_id, host=host, port=port,\n",
    "                         working_directory=working_dir)\n",
    "    ns_host, ns_port = ns.start()\n",
    "\n",
    "    # Start local worker\n",
    "    w = PyTorchWorker(run_id=run_id, host=host, nameserver=ns_host,\n",
    "                      nameserver_port=ns_port, timeout=120)\n",
    "    w.run(background=True)\n",
    "\n",
    "    # Run an optimizer\n",
    "    bohb = BOHB(configspace=worker.get_configspace(),\n",
    "                run_id=run_id,\n",
    "                host=host,\n",
    "                nameserver=ns_host,\n",
    "                nameserver_port=ns_port,\n",
    "                min_budget=min_budget, max_budget=max_budget)\n",
    "    \n",
    "    result = bohb.run(n_iterations=n_bohb_iterations)\n",
    "    print(\"Write result to file {}\".format(result_file))\n",
    "    with open(result_file, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "finally:\n",
    "    bohb.shutdown(shutdown_workers=True)\n",
    "    ns.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate result\n",
    "\n",
    "The result object which we dumped to disk contains all the runs with the different configurations.\n",
    "Here we will analyse it further. The [HpBandSter Analysis Example](https://automl.github.io/HpBandSter/build/html/auto_examples/plot_example_6_analysis.html) is there, if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load a saved result object if necessary\n",
    "with open(result_file, 'rb') as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Print the model of the best run, evaluated on the largest budget, with it's final validation error. *Tipp:* Have a look at the [HpBandSter Result Docs](https://automl.github.io/HpBandSter/build/html/core/result.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "\n",
    "# END TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can gain deeper insight through plotting results. Thanks to the [HpBandSter Visualization Module](https://automl.github.io/HpBandSter/build/html/core/visualization.html) plotting is a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpbandster.visualization as hpvis\n",
    "\n",
    "all_runs = result.get_all_runs()\n",
    "id2conf = result.get_id2config_mapping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see, if we really can evaluate more configurations when makeing use of low budget runs.\n",
    "\n",
    "**Task:** Plot the finished runs over time. How many runs per minute did finish for the individual budgets (only approximately)? \n",
    "\n",
    "**Answer:** **TODO** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ##################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating configurations on lower budgets doesn't make sense - even if they are faster - if the performance ranking isn't consistent from low to high budget. This means, that the loss rankings for configurations should correlate. In simplified terms: The best configuration after one epoch should also be the best after nine epochs, the second best should stay the second best and so on.\n",
    "\n",
    "**Task:** Plot correlations of rankings across budgets. Are the correlations high enough?\n",
    "\n",
    "**Answer:** **TODO** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ##################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually assume, that training on a higher budget (number of epochs) and sampling more configurations can lead to better results. Let's check this.\n",
    "\n",
    "**Task:** Plot the losses over time. Do our assumptions hold true? Why?\n",
    "\n",
    "**Answer:** **TODO**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ##################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that BOHB uses a model after some time to improve the configuation sampling. We can check, if the BO-sampled configurations work better than the random-sampled.\n",
    "\n",
    "**Task:** Plot loss histograms for all budgets only with BO-sampled and only with random-sampled configurations (6 histograms). Is the BO-sampling useful?\n",
    "\n",
    "**Answer:** **TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ##################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 6.2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
