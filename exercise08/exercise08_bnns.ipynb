{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eighth Exercise (BNNs and Ensembles)\n",
    "\n",
    "This exercise focuses on Uncertainty in Deep Learning.\n",
    "\n",
    "We will\n",
    "- Implement an optimizer that uses a Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) method\n",
    "- Implement a BNN\n",
    "- See how uncertainty estimates can be derived using BNNs\n",
    "- Plot these uncertainty estimates\n",
    "- Implement an ensemble of NNs to get their uncertainty estimates and plot them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Bayesian inference in BNNs\n",
    "\n",
    "\n",
    "BNNs define a distribution over the parameters of an NN. We then use the Bayes rule to calculate the posterior distribution over the parameters $\\mathbf{\\theta}$: \n",
    "<br><br>\n",
    "    \\begin{equation}\n",
    " \\quad\\quad   \\quad\\quad        p(\\mathbf{\\theta} \\vert \\mathbf{D})  = \\frac{p(\\mathbf{D} \\vert \\mathbf{\\theta}) \\cdot p(\\mathbf{\\theta})}{p(\\mathbf{D})}  \\quad.......\\quad (\\textbf{Eqn. 1})\n",
    "    \\end{equation}\n",
    "    \n",
    "where $\\mathbf{D}$ is the data, $\\mathbf{\\theta}$ are the parameters.\n",
    "\n",
    "Averaging the predictions over the posterior gives us the mean predictions of the BNN:\n",
    "<br><br>\n",
    "    \\begin{equation*}\n",
    "        \\mathbf{\\mu}_{BNN}(\\mathbf{x}) = \\intop BNN(\\mathbf{x}; \\mathbf{\\theta}) \\, p(\\mathbf{\\theta} \\vert \\mathbf{D}) \\, d\\mathbf{\\theta}  \\quad.......\\quad (\\textbf{Eqn. 2})\n",
    "    \\end{equation*}\n",
    "    \n",
    "and the variance in the predictions:\n",
    "<br><br>\n",
    "    \\begin{equation*}\n",
    "        \\mathbf{\\sigma^2}_{BNN}(\\mathbf{x}) = \\intop \\Big( BNN(\\mathbf{x}; \\mathbf{\\theta}) - \\mathbf{\\mu}_{BNN}(\\mathbf{x}) \\Big)^2 \\, p(\\mathbf{\\theta} \\vert \\mathbf{D}) \\, d\\mathbf{\\theta}  \\quad.......\\quad (\\textbf{Eqn. 3})\n",
    "    \\end{equation*}\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- a) What does the averaging above achieve with regard to the generalisation capabilities of the network?\n",
    "- b) Why would the averaging above usually be intractable for Neural Networks?\n",
    "\n",
    "**Answers:** **TODO**\n",
    "- a) The generalisation capabilities improve when we calculate the mean over the predictions, because we calculated the mean over all possible BNN, which is not sensitive to outliers of individual networks.\n",
    "\n",
    "- b) We would have to calculate p(D) with integrating over all thetas. Furthermore the prior and the posterior are not conjugate.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 SG-MCMC methods for BNNs\n",
    "\n",
    "Since exact inference in BNNs is usually intractable, here we will use Markov Chain Monte Carlo (MCMC) methods to average over sampled network parameters. Combining these with Stochastic Gradient methods gives us batch methods to update network parameters for BNNs and at the same time be able to sample them. Here we will implement a simple SG-MCMC method, preconditioned Stochastic Gradient Langevin Dynamics (pSGLD) from http://people.ee.duke.edu/~lcarin/aaai_psgld_final.pdf which basically preconditions the original SGLD from https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf by combining it with RMSProp.<br><br>**Note:** You don't have to read the papers to finish the exercise, the necessary equations adapted for our use are provided here.\n",
    "\n",
    "#### 8.1.1.1 Stochastic Gradient Langevin Dynamics (SGLD)\n",
    "\n",
    "In SGLD, we insert a bit of noise into the update step from SGD. We draw this noise from the normal distribution with a variance equal to the current learning rate. The update step for the parameters at time step $t$ then looks like:\n",
    "<br><br>\n",
    "    \\begin{align}\n",
    "\\text{Estimate mini-batch gradient per sample:}\\quad& \\bar{\\mathbf{g}}(\\mathbf{\\theta}_t;\\mathbf{D}^{t}) \\leftarrow -\\underbrace{ \\frac{1}{N} \\nabla_{\\mathbf{\\theta}_t} \\text{log}\\, p(\\mathbf{\\theta}_t)}_{\\text{Derivative of log prior}} - \\underbrace{\\frac{1}{n} \\sum^{n}_{i=1} \\nabla_{\\mathbf{\\theta}_t} \\text{log}\\, p(\\mathbf{d}_{t_i}|\\mathbf{\\theta}_t)}_{\\text{Derivative of log likelihood}}   \\quad.......\\quad (\\textbf{Eqn. 4})\\\\\n",
    "\\text{Update parameters:}\\quad& \\mathbf{\\theta}_{t+1} \\leftarrow \\mathbf{\\theta}_{t} - \\frac{\\epsilon_t}{2} \\big(N\\bar{\\mathbf{g}}(\\mathbf{\\theta}_t;\\mathbf{D}^{t})\\big) - \\underbrace{\\mathcal{N}(0, \\epsilon_t \\mathbf{I})}_{\\text{Injected noise}}   \\quad.......\\quad (\\textbf{Eqn. 5})\\\\\n",
    "    \\end{align}\n",
    "where: <br> $n$ is the mini-batch size, <br>\n",
    "$N$ is the total number of data points, <br>\n",
    "$\\mathbf{D}^{t} = \\{\\mathbf{d}_{t_1}, ..., \\mathbf{d}_{t_n}\\}$ is a randomly selected mini-batch of the data, \n",
    "<br>$\\epsilon_t$ is the learning rate at time step $t$.\n",
    "\n",
    "This is the normal stochastic gradient descent update step when the loss is the **negative log likelihood (NLL)** and with a prior distribution $p(\\mathbf{\\theta})$ imposed on the parameters with additional noise injected into the update step.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- a) Assume we are maximising the posterior probability for the parameters given the data (i.e., Eqn. 1). Derive the SGLD update step (i.e., Eqn. 5) from that. <br>(**Note 1:** The learning rate is being divided by 2 here. **Note 2:** Typeset the answer in Latex.)\n",
    "\n",
    "**Answers:** **TODO**\n",
    "Bayes Rule mit neg Log \n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Initially, the noise in the stochastic gradient estimate dominates the injected noise in the update step but later on the injected noise dominates and the updated parameters at each step correspond to samples from the posterior distribution of the parameters (derivations in the original SGLD paper for those interested). We call the initial phase **burn-in** and the latter phase the **Langevin dynamics** phase. We can judge the end of the burn-in phase based on a sampling threshold calculated in the original SGLD paper. But for simplicity, here we use an empirically defined number of update steps to denote the burn-in phase.\n",
    "\n",
    "#### 8.1.1.2 preconditioned Stochastic Gradient Langevin Dynamics (pSGLD)\n",
    "\n",
    "pSGLD tries to improve upon SGLD by additionally using a preconditioner matrix for the update step to account for different curvatures in different dimensions. To implement pSGLD here, we inject noise similar to SGLD and precondition the update step with the preconditioner from RMSProp. For each time step $t$:\n",
    "<br><br>\n",
    "    \\begin{align*}\n",
    "\\text{Estimate average of squared gradient:}\\quad& \\mathbf{V}(\\mathbf{\\theta}_t) \\leftarrow \\alpha \\mathbf{V}(\\mathbf{\\theta}_{t-1}) + (1 - \\alpha)\\,\\bar{\\mathbf{g}}(\\mathbf{\\theta}_t;\\mathbf{D}^{t}) * \\bar{\\mathbf{g}}(\\mathbf{\\theta}_t;\\mathbf{D}^{t}) \\quad.......\\quad (\\textbf{Eqn. 6})\\\\\n",
    "\\text{Estimate preconditioning matrix:}\\quad& \\mathbf{G}(\\mathbf{\\theta}_t) \\leftarrow diag( \\mathbf{1}\\, / \\,(\\lambda\\, \\mathbf{1} + \\sqrt{\\mathbf{V}(\\mathbf{\\theta}_{t}} )) \\quad.......\\quad (\\textbf{Eqn. 7})\\\\\n",
    "\\text{Update parameters:}\\quad& \\mathbf{\\theta}_{t+1} \\leftarrow \\mathbf{\\theta}_{t} - \\frac{\\epsilon_t}{2} (\\mathbf{G}(\\mathbf{\\theta}_t) \\big( N\\bar{\\mathbf{g}}(\\mathbf{\\theta}_t;\\mathbf{D}^{t})\\big) - \\mathcal{N}(0, \\epsilon_t (\\mathbf{G}(\\mathbf{\\theta}_t)) \\quad.......\\quad (\\textbf{Eqn. 8})\\\\\n",
    "    \\end{align*}\n",
    "\n",
    "where: \n",
    "<br>$\\alpha \\in [0, 1]$ decides how much to weight old information in the exponential moving average of the squared gradient, <br>$\\lambda$ is a small constant to avoid divison by zero, <br>$\\mathbf{V}(\\mathbf{\\theta}_0) = \\mathbf{0}$, <br>$\\mathbf{1}$ is a vector with all $1s$ <br>and where required operations are carried out element-wise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell does some initial setup for this exercise. \n",
    "\n",
    "**Task:** \n",
    "Implement a pSGLD optimizer in the cell after the next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some initial setup: Importing required libraries\n",
    "import os\n",
    "from typing import Tuple, List, Iterable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def minibatched(data: np.ndarray, batch_size: int) -> List[np.ndarray]:\n",
    "    \"\"\"Mini-batchifies data\"\"\"\n",
    "    assert len(data) % batch_size == 0, (\"Data length {} is not multiple of batch size {}\"\n",
    "                                         .format(len(data), batch_size))\n",
    "    return data.reshape(-1, batch_size, *data.shape[1:])\n",
    "\n",
    "def evaluate_loss(model: nn.Module, loss_func: nn.Module, \n",
    "                  x: torch.Tensor, y: torch.Tensor) -> Tuple[float, float]:\n",
    "    \"\"\"Evaluates given loss function for given data for the given model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    loss = []\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        loss = loss_func(output, y)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class pSGLD(torch.optim.Optimizer):\n",
    "    \"\"\"preconditioned Stochastic Gradient Langevin Dynamics (pSGLD) Optimizer\"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[nn.Parameter], lr: float, num_train_points: int,\n",
    "                 alpha: float, lamb: float):\n",
    "        \"\"\"Init for various parameters of pSGLD, i.e., the hyperparams\n",
    "        \"\"\"\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "\n",
    "        defaults = dict(lr=lr, num_train_points=num_train_points,\n",
    "                        alpha=alpha, lamb=lamb)\n",
    "        super(pSGLD, self).__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Implementation is similar to Pytorch's implementation of SGD here:\n",
    "        https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD\n",
    "        \"\"\"\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                \n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                state = self.state[p]\n",
    "                lr = group[\"lr\"]\n",
    "                num_train_points = group[\"num_train_points\"]\n",
    "                alpha = group[\"alpha\"]  # alpha\n",
    "                lamb = group[\"lamb\"]  # lambda\n",
    "                gradient = p.grad.data\n",
    "\n",
    "                #  state initialization\n",
    "                if len(state) == 0:\n",
    "                    # START TODO ################\n",
    "                    # Init V(theta_t)\n",
    "                    # END TODO ################\n",
    "\n",
    "                # START TODO ################\n",
    "                # Carry out the steps of pSGLD: Eqn. 6, 7 and 8\n",
    "                # The gradient which is used here comes from eqn. 4. But you don't have\n",
    "                # to worry about calculating it here. It will be auto-calculated by pytorch once \n",
    "                # you implement the NLLWithPriorLoss and use it later in the exercise.\n",
    "                # END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 Implement a BNN\n",
    "\n",
    "**Question:**\n",
    "- a) How would you modify the calculation of the mean and variance (from section 8.1 above) to get empirical estimates of those quantities for the BNN? (**Note:** Typeset the answer in Latex)\n",
    "\n",
    "**Answer: TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BNN(nn.Module):    \n",
    "    def __init__(self, model: nn.Module, burn_in: int, keep_every: int):\n",
    "        \"\"\"Initializes various parameters of the BNN\n",
    "        \n",
    "        model: Any neural network\n",
    "        burn_in: Number of steps of the burn-in phase\n",
    "        keep_every: Save a sample of the held network's parameters \n",
    "                    every keep_every steps after the burn-in phase\n",
    "                    \n",
    "        \"\"\"\n",
    "        super(BNN, self).__init__()\n",
    "        # START TODO ########################\n",
    "        # Maintain a state for the BNN including the current step\n",
    "        # and currently held sampled network parameters.\n",
    "        # What to code here will probably be clearer\n",
    "        # after implementing the step() function below\n",
    "        # END TODO ########################\n",
    "\n",
    "    # Since the BNN has to take samples of the networks, we implement a step() function for the BNN as well.        \n",
    "    def step(self):\n",
    "        \"\"\"Implements the step function of the BNN\"\"\"\n",
    "        # START TODO ########################\n",
    "        # Sample the held neural network's parameters here by saving them to the BNN's state\n",
    "        # Note: It'd be easier to just save a deepcopy of the whole neural network.\n",
    "        # Remember to sample only after the burn-in phase is done and then sample every\n",
    "        # keep_every steps\n",
    "        # END TODO ########################\n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the BNN\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict_mean_and_std(self, x, \n",
    "                             return_individual_predictions: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns mean and standard deviation of the predictions of the sampled networks\n",
    "        \n",
    "        In case return_individual_predictions is True, return all the predictions of the sampled networks, \n",
    "        as a Torch tensor\n",
    "        \"\"\"\n",
    "        # START TODO ########################\n",
    "        # When training or before the burn-in phase is completed,\n",
    "        # we don't intend to get uncertainty estimates in the\n",
    "        # predictions anyway, so just return the output from\n",
    "        # the normal forward pass and the standard deviation as 0, \n",
    "        # otherwise return the mean of the predictions and standard deviation\n",
    "        # of the sampled networks.\n",
    "\n",
    "        # We will be predicting the mean and the log variance of the data (see slide 47 of lecture)\n",
    "        # in later code below. So the first dimension of the netowrk outputs will be the mean\n",
    "        # and the 2nd dimension will be the log variance\n",
    "        # which amounts to the observational noise. So, don't forget\n",
    "        # to add the variances from the predicted variance (i.e., exp of 2nd dimension of output)\n",
    "        # to the empirical variance of the mean (i.e., variance of 1st dimension of output) \n",
    "        # according to the law of total variance. Be extra careful with where you take squares\n",
    "        # and exp()s, etc.!!\n",
    "\n",
    "        # In case, return_individual_predictions is True, return\n",
    "        # all the predictions of all the sampled networks.\n",
    "        # END TODO ########################\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3 Sinc function\n",
    "\n",
    "We will be performing regession on the sinc function. It is defined as:\n",
    "<br><br>\n",
    "    \\begin{equation*}\n",
    "        sinc(x) = \\frac{sin(x)}{x}\n",
    "    \\end{equation*}\n",
    "    \n",
    "The value at x = 0 is defined to be the limiting value, equal to 1.\n",
    "\n",
    "**Task:**\n",
    "Please complete the code to plot the function in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# START TODO ########################\n",
    "# Note: You can use the sinc implementation from numpy\n",
    "# END TODO ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.4 Loss function\n",
    "\n",
    "We will be minimizing the negative log likelihood (NLL) loss + prior function in our algorithm. Here we will model this prior using a normal distribution and impose this prior only on the weights of the network:\n",
    "<br><br>\n",
    "    \\begin{equation}\n",
    "        p(\\mathbf{\\theta}) = \\mathcal{N}(0, \\mathbf{\\sigma}^{2}_{\\mathbf{\\theta}})  \\quad.......\\quad (\\textbf{Eqn. 9})\n",
    "    \\end{equation}\n",
    "<br>\n",
    "We set this $\\mathbf{\\sigma}^{2}_{\\mathbf{\\theta}}$ equal to the constant 1 here.\n",
    "<br>\n",
    "**Note:** This variance on the parameters of the network is different from the variance of the output.\n",
    "<br>\n",
    "\n",
    "In order to use the NLL function we need a probability density for the output of the model given its inputs and parameters. Here we will model this probability using a normal distribution. The normal distribution is completely characterised by its mean and variance. So, we will predict the mean and variance of the output given the input, i.e., the mean of this distirubtion is given by the empirical estimate of eqn. 2 above and the variance is given by the empirical estimate of eqn. 3 above.\n",
    "<br>\n",
    "\n",
    "If the variance of output doesn't depend on the input, then we have **homoscedasticity**. Otherwise, we have heteroscedasticity. To keep it simple here, we model our data to be homoscedastic.\n",
    "<br>\n",
    "\n",
    "In the next cell, we will implement the NLL loss with a prior and also a homoscedastic layer for a neural network.\n",
    "\n",
    "**Questions:**\n",
    "- a) Why can't we use the NLLLoss implemented in PyTorch?\n",
    "- b) Derive the log prior component of the loss with respect to the neural network's parameters when modelling the prior as the normal distribution above with $\\mathbf{\\sigma}^{2}_{\\mathbf{\\theta}}$ = 1. <br>(**Note:** It's the derivative of the log prior from Eqn. 4 above that needs to be found in the specific case when the prior is a normal distribution.)\n",
    "- c) Why would minimizing the NLL loss be equivalent to minimising the MSE loss for homoscedastic data but not necessarily for heteroscedastic data? <br>(**Note:** No need to derive the whole thing, just an answer in words is enough)\n",
    "\n",
    "**Answers: TODO**\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "**Task:**\n",
    "Please implement the missing parts of the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NLLWithPriorLoss(nn.Module):\n",
    "    def __init__(self, params: Iterable[nn.Parameter], num_train_points: int):\n",
    "        \"\"\"Init for NLLWithPriorLoss\n",
    "        \n",
    "        We need num_train_points because the log prior component from eqn. 4\n",
    "        is divided by num_train_points.\n",
    "\n",
    "        We need params to calculate the log prior component in eqn. 4.\n",
    "        Remember to instantiate an object of this class with model.named_parameters() \n",
    "        so that we can check for which parameters to use the prior. In this case, we \n",
    "        impose a Gaussian prior only on the weight parameters.\n",
    "        \"\"\"\n",
    "        super(NLLWithPriorLoss, self).__init__()\n",
    "        self.parameters = list(params)\n",
    "        self.num_train_points = num_train_points\n",
    "\n",
    "    def forward(self, input, target) -> torch.Tensor:\n",
    "        \"\"\"Implement forward for the loss function.\n",
    "        \n",
    "        Perform the Stochastic Gradient and log prior calculations from eqn. 4.\n",
    "        Don't forget to divide the log likelihood by batch_size and to divide the log\n",
    "        prior by the num_train_points. These will be rescaled by num_train_points\n",
    "        in the pSGLD optimizer.\n",
    "        \n",
    "        Please remember that the neural net has 2 outputs: the mean and the log variance.\n",
    "        \"\"\"\n",
    "        # START TODO ########################\n",
    "\n",
    "        # END TODO ########################        \n",
    "        return -log_likelihood\n",
    "\n",
    "class HomoscedasticLogVar(nn.Module):\n",
    "    \"\"\"This will be last layer of the Neural Network we train.\n",
    "    It takes the normal output of the neural network until that point,\n",
    "    i.e. the mean prediction, and just appends a vector containing the\n",
    "    log variance which best explains the data, i.e., which minimizes the loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, logvar, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logvar = nn.Parameter(torch.FloatTensor([[logvar]]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # START TODO ########################\n",
    "        # Concatenate the predicted log variance with x,\n",
    "        # This will result in 2 outputs for the neural net\n",
    "        # when we append this layer to a normal NN with one output.\n",
    "\n",
    "        # END TODO ########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.5 Evaluation\n",
    "\n",
    "**Task:**\n",
    "Implement parts of the evaluate_model() function which takes a BNN along with an optimizer and a learning rate scheduler and trains the BNN in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, optimizer, loss_func, scheduler, x_train, y_train, x_test, y_test, epochs, batch_size):\n",
    "    print(\"Training and evaluating model...\")\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(int(epochs)):\n",
    "        print(\"  Epoch {} / {} ...\".format(epoch + 1, epochs).ljust(2))\n",
    "        # START TODO ########################\n",
    "        # END TODO ########################\n",
    "\n",
    "        ix = np.arange(len(x_train))\n",
    "        np.random.shuffle(ix)\n",
    "        \n",
    "        shuffled_data = zip(minibatched(x_train[ix], batch_size), minibatched(y_train[ix], batch_size))\n",
    "\n",
    "        for i, (x, y) in enumerate(shuffled_data):\n",
    "            # START TODO ########################\n",
    "            # Don't forget to call model.step() in case you call this function on the BNN from above\n",
    "            # END TODO ########################\n",
    "        train_loss = evaluate_loss(model, loss_func, x_train, y_train)\n",
    "        test_loss = evaluate_loss(model, loss_func, x_test, y_test)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        print('Train loss:', train_losses[-1], 'Test loss:', test_losses[-1])\n",
    "\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Now write the code which uses the above code to train the BNN. The scheduler needs to have a decreasing schedule for the learning rate, otherwise the algorithm may not converge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We have selected the hyperparamters for you below.\n",
    "n_train = 30\n",
    "epochs = 10000\n",
    "batch_size=n_train//2\n",
    "lr = 1e-3\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.uniform(0, 1, n_train)\n",
    "y = np.sinc(x * 10 - 5)\n",
    "\n",
    "x = torch.FloatTensor(x[:, None])\n",
    "y = torch.FloatTensor(y[:, None])\n",
    "\n",
    "x_test = np.random.uniform(0, 1, 20)\n",
    "y_test = np.sinc(x_test * 10 - 5)\n",
    "x_test = torch.from_numpy(x_test[:, None]).float()\n",
    "y_test = torch.from_numpy(y_test[:, None]).float()\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(in_features=1, out_features=50, bias=True),\n",
    "  nn.Tanh(),\n",
    "  nn.Linear(in_features=50, out_features=50, bias=True),\n",
    "  nn.Tanh(),\n",
    "  nn.Linear(in_features=50, out_features=50, bias=True),\n",
    "  nn.Tanh(),\n",
    "  nn.Linear(in_features=50, out_features=1, bias=True),\n",
    "  HomoscedasticLogVar(np.log(1e-3))\n",
    ")\n",
    "\n",
    "# Note: The number of burn-in steps is the total number of mini-batch steps\n",
    "# while the number of epochs counts passes over the whole batch\n",
    "bnn = BNN(model, burn_in=15000, keep_every=50)\n",
    "criterion = NLLWithPriorLoss(bnn.named_parameters(), num_train_points=n_train)\n",
    "optimizer = pSGLD(bnn.parameters(), lr=lr, num_train_points=n_train, \n",
    "                  alpha=0.99, lamb=1e-5)\n",
    "\n",
    "lambda1 = lambda epoch: 1/(epoch//20 + 1)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "# START TODO ########################\n",
    "# END TODO ########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.6 Plotting\n",
    "\n",
    "**Task:**\n",
    "Plot the training and test curves for the BNN runs from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# START TODO ########################\n",
    "# With NLL loss, the initial losses are really huge compared to the later one\n",
    "# so plot only from, say, the 10th loss onwards.\n",
    "# END TODO ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "- a) What do you observe regarding the test loss later on? What does this tell you?\n",
    "\n",
    "**Answer: TODO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Plot uncertainties in the predictions of the BNN below. Use fill_between() from matplotlib to fill in the area around the mean covered by 1, 2 and 3 standard deviations. Vary the alpha (opacity) values of these to get a good visualization of the uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_uncertainty(model):\n",
    "    xlim = (-0, 1)\n",
    "    ylim = (-2, 2)\n",
    "    grid = np.linspace(*xlim, 200)\n",
    "    # START TODO ########################\n",
    "    # Use the predict_mean_and_std() function of the BNN to get mean predictions\n",
    "    # and standard deviations to plot uncertainties. Plotting mean prediction using plot()\n",
    "    # and fill_between() from matplotlib should be helpful\n",
    "    # END TODO ########################\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('x value')\n",
    "    plt.ylabel('y value')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_uncertainty(bnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "- a) What do you observe in the graph?\n",
    "\n",
    "**Answer: TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Plot the predictions of the individual networks (i.e., the networks with the different sets of parameters from the posterior) of the BNN below. Plot each prediction with a smaller alpha (opacity) so that we can get an idea of the predictions of all the individual networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_multiple_predictions(model):\n",
    "    # START TODO ########################\n",
    "    # Use the predict_mean_and_std() function of the BNN with return_individual_predictions = True\n",
    "    # to get individual predictions for each of the sampled set of parameters of the BNN.\n",
    "    # Plot all of these one by one using plot() with some transparency using its alpha parameter\n",
    "    # so that you get a good visualization of the individual predictions\n",
    "    # END TODO ########################\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('x value')\n",
    "    plt.ylabel('y value')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_multiple_predictions(bnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 8.1\n",
    "**pSGLD:<br>\n",
    "BNN:<br>\n",
    "NLLWithPriorLoss and HomoscedasticLogVar:<br>\n",
    "Plotting:<br>\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Uncertainties using Ensembles\n",
    "\n",
    "**Task:**\n",
    "Complete the code for training an ensemble of NNs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnsembleSequential(nn.Sequential):\n",
    "    \"\"\"Holds an ensemble of NNs which are used to get prediction means and uncertainties\n",
    "    similar to the BNNs above\"\"\"\n",
    "    def __init__(self, *args, num_models=5, ensembled_nets, **kwargs):\n",
    "        \"\"\"Init for the class\"\"\"\n",
    "        super(EnsembleSequential, self).__init__(*args, **kwargs)\n",
    "        self.num_models = num_models\n",
    "        self.ensembled_nets = ensembled_nets\n",
    "    \n",
    "    def predict_mean_and_std(self, x, \n",
    "                             return_individual_predictions: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns mean and standard deviation of the predictions of the ensembled networks\n",
    "        \n",
    "        In case return_individual_predictions is True, return all the predictions of the ensembled networks, \n",
    "        as a Torch tensor\n",
    "        \"\"\"\n",
    "        # START TODO ########################\n",
    "        # The code here is very similar to the predict_mean_and_std() from the BNNs above.\n",
    "        # END TODO ########################\n",
    "        return mean, std\n",
    "\n",
    "ensembled_nets = []\n",
    "num_models=5\n",
    "\n",
    "# The initialization of the weights turns out to be crucial for getting good uncertainty estimates,\n",
    "# so we do that here.\n",
    "def init_weights(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight, mean=0, std=2)\n",
    "        nn.init.constant_(module.bias, val=0.0)\n",
    "\n",
    "\n",
    "for i in np.arange(num_models):\n",
    "    temp_model = nn.Sequential(\n",
    "      nn.Linear(in_features=1, out_features=50, bias=True),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(in_features=50, out_features=50, bias=True),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(in_features=50, out_features=1, bias=True),\n",
    "    ).apply(init_weights)\n",
    "    ensembled_nets.append(temp_model)\n",
    "\n",
    "ensemble = EnsembleSequential(ensembled_nets=ensembled_nets)\n",
    "\n",
    "for i in np.arange(num_models):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    epochs=10000\n",
    "    lr = 1e-2\n",
    "    optimizer = torch.optim.SGD(ensemble.ensembled_nets[i].parameters(), lr=lr)\n",
    "    lambda1 = lambda epoch: 1.\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "    # START TODO ########################\n",
    "    # END TODO ########################\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_uncertainty(ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_multiple_predictions(ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![christmas-new-year3.jpg](attachment:christmas-new-year3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 8.2\n",
    "**Ensembles:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
